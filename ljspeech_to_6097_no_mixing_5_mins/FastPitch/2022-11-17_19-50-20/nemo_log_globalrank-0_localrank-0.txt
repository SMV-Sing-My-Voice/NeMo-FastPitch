[NeMo W 2022-11-17 19:50:00 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-11-17 19:50:01 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-11-17 19:50:01 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-11-17 19:50:01 cloud:56] Found existing object /Users/euna/.cache/torch/NeMo/NeMo_1.13.0rc0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.
[NeMo I 2022-11-17 19:50:01 cloud:62] Re-using file from: /Users/euna/.cache/torch/NeMo/NeMo_1.13.0rc0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo
[NeMo I 2022-11-17 19:50:01 common:911] Instantiating model from pre-trained checkpoint
[NeMo I 2022-11-17 19:50:03 tokenize_and_classify:87] Creating ClassifyFst grammars.
[NeMo W 2022-11-17 19:50:18 experimental:27] Module <class 'nemo_text_processing.g2p.modules.IPAG2P'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-11-17 19:50:18 modules:95] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.
[NeMo W 2022-11-17 19:50:18 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_train_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: 0.1
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: 24
      num_workers: 0
    
[NeMo W 2022-11-17 19:50:18 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_val_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: null
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: 24
      num_workers: 0
    
[NeMo W 2022-11-17 19:50:19 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/torch/jit/annotations.py:299: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.
      warnings.warn("TorchScript will treat type annotations of Tensor "
    
[NeMo I 2022-11-17 19:50:19 features:225] PADDING: 1
[NeMo I 2022-11-17 19:50:19 save_restore_connector:243] Model FastPitchModel was successfully restored from /Users/euna/.cache/torch/NeMo/NeMo_1.13.0rc0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.
[NeMo W 2022-11-17 19:50:19 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo W 2022-11-17 19:50:19 fastpitch_finetune:28] You are using an optimizer scheduler while finetuning. Are you sure this is intended?
[NeMo W 2022-11-17 19:50:20 nemo_logging:349] /Users/euna/opt/anaconda3/envs/NeMo-FastPitch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:704: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.
      rank_zero_warn(
    
[NeMo I 2022-11-17 19:50:20 exp_manager:315] Experiments will be logged at ljspeech_to_6097_no_mixing_5_mins/FastPitch/2022-11-17_19-50-20
[NeMo I 2022-11-17 19:50:20 exp_manager:704] TensorboardLogger has been set up
[NeMo W 2022-11-17 19:50:20 exp_manager:971] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 25 epochs to ensure that checkpointing will not error out.
